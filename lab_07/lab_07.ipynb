{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas | Core concepts, types and methods (part II)\n",
    "\n",
    "Szymon Talaga 10.01.2020\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we continue our quest for developing solid, in-depth understanding of Pandas data structures and their relations to Numpy.\n",
    "\n",
    "Here we will focus on the most important type which is `DataFrame`. In the later part we will also consider `MultiIndex` type\n",
    "and advanced methods for indexing hierarchical data structures. In the first part of the notebook we will focus on flat indexes\n",
    "with only one level (i.e. standard `Index` objects).\n",
    "\n",
    "Data frame in Pandas is a two dimensional collection of values arranged as a rectangular grid of rows and columns.\n",
    "It is primarily oriented column-wise as the columns are represented as `Series` objects. This means that any single column of a `DataFrame` has to be\n",
    "of a fixed `dtype`, but different columns may have different `dtypes`. This is a primary reason why (usually) row-wise operations in Pandas\n",
    "are less efficient and in general more difficult to carry out.\n",
    "\n",
    "Data frames use also a more complex indexing architecture. All `Series` defining columns have to share the same index. This allows to have a well-defined\n",
    "row index. Additionally, a data frame has also a horizontal index which defines columns (and possibly also groups of columns etc.).\n",
    "\n",
    "Summing up, data frames can be viewed as a collection of columns represented as `Series` which are mapped to column names organized as an `Index` \n",
    "(or `MultiIndex`) object combined with another `Index` (or `MultiIndex`) object that define row indexes and which is shared by all column `Series`.\n",
    "\n",
    "Although the abovementioned view is not 100% veridical with respect to the true internal data model used by Pandas it rather captures the general\n",
    "design of Pandas data frames. It also correctly points to the strong and weak points of Pandas data model in terms of what kinds of operations\n",
    "are easy and hard to perform (in terms of computational efficiency).\n",
    "\n",
    "```python\n",
    "=====================================================\n",
    "|       | column 1    column 2   . . .     column m |\n",
    "=====================================================\n",
    "| row 1 |    x           x                    x     |\n",
    "| row 2 |    x           x                    x     |\n",
    "|   .   |                                           |\n",
    "|   .   |                                           |\n",
    "|   .   |                                           |\n",
    "| row n |    x           x                    x     |\n",
    "=====================================================\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "Many great resources about Pandas can be found in the official documentation. In particular, it is recommended to read the following articles:\n",
    "\n",
    "* [10 minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n",
    "* [Essential basic functionality](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html)\n",
    "* [Intro to data structures](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html)\n",
    "* [Indexing and selecting data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-attribute-access)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Helper package to load example datasets\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure IPython shell to show print all outputs generated in a code cell\n",
    "### --------------------------------------------------------------------------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` | 2D rectangular dataset organized as a collection of named `Series`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will review most important ways in which one can initialize a `DataFrame` object. \n",
    "This will provide not only a practical exercise but also important insights into the ways in which we can think about data frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most natural, although not necessarily the best, way to think about a data frame is to think about it\n",
    "in terms of a list of rows defining a rectangular table.\n",
    "\n",
    "In this spirit, we can build a data frame from a list of lists defining rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6, 4],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "], columns=('a', 'b', 'c', 'd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we will not get any column or row names by default. However, this can be changed by using `columns` and `index` arugments.\n",
    "\n",
    "Note that we do not have to pass type homogeneous data. Below we create a data frame of which third column is of `object` type\n",
    "(it contains strings) while the first two ones are integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    [1, 2, 'a string 1'],\n",
    "    [4, 5, 'a string 2'],\n",
    "    [7, 8, 'a string 3'],\n",
    "    [10, 11, 'a string 4']\n",
    "], columns=['a', 'b', 'c'], index=['p', 'r', 's', 't'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check types using `.dtypes` attribute of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can look at row and column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index    # row index\n",
    "df.columns  # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach is to create a data frame from a sequence of dictionaries. This way we can specify column names in data.\n",
    "However, if we want to have non-generic row labels we still need to provide them separately through `index` argument.\n",
    "\n",
    "NOTE. The ordering of columns is defined by the ordering of keys in the first dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {'b': 'string', 'a': 2, 'c': 11111},\n",
    "    {'a': 1, 'b': 'another string'},\n",
    "], index=['s1', 's2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fully define data and both indexes in a row-wise fashion if we define a data frame based on a list of named `Series`\n",
    "which are interpreted as rows in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    pd.Series({'a': 1, 'b': 2}, name='subject 1'),\n",
    "    pd.Series({'a': 2, 'b': 3}, name='subject 2')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the column perspective we can define a data frame as a named collection (a mapping) of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [4, 5, 6],\n",
    "    'c': ['x', 'y', 'z']\n",
    "}, index=['s1', 's2', 's3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see that pandes really enforces equal length of columns as well as identical indexes, we can check what happens\n",
    "if try to create data frame from a collection non-conformable series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal lengths\n",
    "pd.DataFrame({\n",
    "    'a': [1, 2, 3, 999],\n",
    "    'b': [4, 5, 6],\n",
    "    'c': ['x', 'y', 'z']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass series with different indexes we do not get an error. Instead we obtain a data frame partially filled with NaNs,\n",
    "which is of course the result of the rules of labels alignment in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal indexes\n",
    "pd.DataFrame({\n",
    "    'a': pd.Series([1, 2, 3]),\n",
    "    'b': pd.Series([4, 5, 6]),\n",
    "    'c': pd.Series(['x', 'y', 'z'], index=['uu', 'vv', 'ww'])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataFrame` | Indexing and slicing\n",
    "\n",
    "`DataFrame` objects provide the same three syntaxes and three kinds of indexing. The main difference is that data frames\n",
    "have two separate axes (rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    pd.Series({'a': 1, 'b': 2, 'c': 'foo'}, name='s1'),\n",
    "    pd.Series({'a': 11, 'b': 22, 'c': 'bar'}, name='s2'),\n",
    "    pd.Series({'a': 20, 'b': 30, 'c': 'xoxo'}, name='s3'),\n",
    "    pd.Series({'a': 7, 'b': 15, 'c': 'yoyo'}, name='s4'),\n",
    "    pd.Series({'a': 50, 'b': 1, 'c': 'howdy'}, name='s5')\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _getitem_ syntax\n",
    "\n",
    "It can be used to select columns (single or multiple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single column\n",
    "# Output is a series\n",
    "df['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple columns\n",
    "# Output is a data frame\n",
    "df[['a', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single column as a data frame\n",
    "df[['b']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we provide a slice in _getitem_ indexing it will be interpreted as slice over row labels\n",
    "or positions if it is an integer slice. Thus, in this case we have a similar problem with ambiguity of the _getitem_ syntax\n",
    "as in the case of `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get slice of rows by label\n",
    "df['s2':'s4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get slice of rows by integer positions\n",
    "df[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguous case of data frame with integer labels\n",
    "df2 = df.set_index(pd.Index([2, 3, 1, 4, 0]))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows by integer slice\n",
    "# They are interpreted as positions and not labels\n",
    "df2[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | `.loc` indexing\n",
    "\n",
    "It provides label-based indexing for both rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row (as a Series)\n",
    "df.loc['s2', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column (as a Series)\n",
    "df.loc[:, 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column (as a DataFrame)\n",
    "df.loc[:, ['b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple columns and rows\n",
    "df.loc[['s2', 's4'], ['a', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get slices of rows and columns\n",
    "df.loc['s2':, :'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | `.iloc` indexing\n",
    "\n",
    "It provides position-based indexing for both rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single element\n",
    "df.iloc[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row (as Series)\n",
    "df.iloc[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row (as DataFrame)\n",
    "df.iloc[[-1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple rows and columns\n",
    "df.iloc[[1, -1], [0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slices\n",
    "df.iloc[:3, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Boolean indexing\n",
    "\n",
    "As in the case of `Series` we can provide 1D boolean masks to filer rows and/or columns.\n",
    "What is important is the fact that with data frames we may mix boolean masks with other types of indexing.\n",
    "For instance, we can have a boolean mask on columns and label-based or positional index on rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.columns.isin(['a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2:, df.columns.isin(['a', 'c'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a'] > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['a'] > 10, ['c', 'a']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new feature is that we can also provide a full boolean mask over entire data frame to mask particular values\n",
    "abd turn them into NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101010)\n",
    "\n",
    "num = pd.DataFrame(np.random.normal(0, 1, (10, 3)), columns=['x', 'y', 'z'])\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask negative values\n",
    "#num >= 0\n",
    "num[num >= 0].sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` | Basic attributes and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .columns\n",
    "## Column index\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .index\n",
    "## Row index\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .dtypes\n",
    "## Column names and their dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## len()\n",
    "## Number of rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .shape\n",
    "## Tuple with number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .info()\n",
    "## Show basic information about a data frame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .describe()\n",
    "## Basic numeric summary of data\n",
    "## Categorical columns are ommited by default if there are any numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## .to_numpy()\n",
    "## Numpy representation\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` | Broadcasting and labels alignment\n",
    "\n",
    "Here we review the broadcasting and labels alignment rules for data frames.\n",
    "In general they are the same as for `Series` objects. However, in this case we have two axes instead\n",
    "of one, so this induces some additional complications.\n",
    "\n",
    "The main rule is that we do labels alignment for both row and column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two simple data frames\n",
    "df1 = pd.DataFrame(np.arange(4).reshape(2, 2), columns=['a', 'b'], index=[3, 7])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(4, 8).reshape(2, 2), columns=['b', 'a'], index=[7, 3])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add them together\n",
    "df2 + df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened step by step\n",
    "row_union = df1.index.union(df2.index)\n",
    "row_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_union = df1.columns.union(df2.columns)\n",
    "col_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reindex(index=row_union, columns=col_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = df1.reindex(index=row_union).reindex(columns=col_union)\n",
    "df1 = df1.reindex(index=row_union, columns=col_union)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reindex(index=row_union, columns=col_union)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final operation\n",
    "df1 + df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what happens if axes can not be perfecly aligned (they are at least partially non-overlapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(8).reshape(4, 2), index=[2, 4, 6, 8], columns=['a', 'b'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(8).reshape(4, 2), index=[2, 3, 4, 5], columns=['b', 'c'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 + df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened step by step\n",
    "row_union = df1.index.union(df2.index)\n",
    "row_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_union = df1.columns.union(df2.columns)\n",
    "col_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reindex(index=row_union, columns=col_union)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reindex(index=row_union, columns=col_union)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final operation\n",
    "df1 + df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how are aligned operations between series and data frames? The rule is that index labels of a series are matched\n",
    "with column labels of a data frame. It makes it easy to define operations based on columns and their aggregate\n",
    "values such as centering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center columns of numeric data frame\n",
    "df = pd.DataFrame(np.random.normal(100, 15, (10, 3)), columns=['x', 'y', 'z'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column means\n",
    "# Mean computed over columns\n",
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df - df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it is really centered\n",
    "df_c.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it as easy carry out operations row-wise? Nope. But it can be done, although it requires some additional tricks.\n",
    "However, first let us see and understand what is going on, when we try to broadcast between data frame and \n",
    "a series representing row-aggregated values (i.e. row centering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row means\n",
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove means from rows\n",
    "df_c = df - df.mean(axis=1)\n",
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total disaster! Do you understand what happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way, although a little convoluted and not really the best one, to deal with this is to use transposition.\n",
    "Data frames are inherently two-dimensional (they have rows and columns) like matrices. So we can easily\n",
    "define a transpose of a data frame. Below is the transpose of our original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this trick in our hands now we can reexpress our row-oriented problem as column oriented problem\n",
    "and convert the result back to the original orientation with yet another transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T\n",
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.T - df.mean(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.T - df.mean(1)).T.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = (df.T - df.mean(1)).T\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it worked\n",
    "df_c.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': [1, 2, 3],\n",
    "    'y': [1., 2., 3.]\n",
    "})\n",
    "df\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.T.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 1.\n",
    "\n",
    "You are provided with a simple numeric data frame. Standardize it both column and row-wise.\n",
    "In other words both column and row means should be $0$ and standard deviations (and variances) should be $1$.\n",
    "\n",
    "Remember that the formula for standardization is the following:\n",
    "\n",
    "$$X_{\\text{standardized}} = \\frac{X - \\text{Mean}(X)}{\\text{Std}(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "df = pd.DataFrame(np.random.normal(100, 15, (10, 3)), columns=['x', 'y', 'z'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexible operations (arithmetic, logical etc.)\n",
    "\n",
    "We managed to implement row-wise operation with some smart use of transposition. However, this seems rather hacky\n",
    "and we would like to have some better tools for doing just that. Happily Pandas provides us with such tools.\n",
    "\n",
    "`DataFrame` and `Series` objects in Pandas implements special methods called _flexible operations_ which are\n",
    "just standard arithmetic and logical operation, but such that can be explicitly applied along a given axis.\n",
    "Moreover, they can also automatically substitute NaNs which are created during labels alignment.\n",
    "\n",
    "**Flexible arithmetic binary operations**\n",
    "\n",
    "1. `add`\n",
    "2. `sub`\n",
    "3. `div`\n",
    "4. `mul`\n",
    "5. `pow`\n",
    "\n",
    "**Flexible logical binary operations**\n",
    "\n",
    "1. `eq` (equal)\n",
    "2. `ne` (not equal)\n",
    "3. `lt` (lower than)\n",
    "4. `gt` (greater than)\n",
    "5. `le` (lower or equal)\n",
    "6. `ge` (greater or equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.normal(100, 15, (10, 2)), columns=['x', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df - df.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLEXIBLE OPERATIONS APPROACH\n",
    "df.sub(df.mean(0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.T - df.mean(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sub(df.mean(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding partially matching series with automatic substitution of NaNs\n",
    "s1 = pd.Series([1, 2, np.nan], index=['a', 'b', 'c'])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series([6, 4, 7, 8], index=['a', 'd', 'f', 'b'])\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_union = s1.index.union(s2.index)\n",
    "s1.reindex(index_union).fillna(0) + s2.reindex(index_union).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.add(s2, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened step by step\n",
    "# Step 1. Index union.\n",
    "index_union = s1.index.union(s2.index)\n",
    "index_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Reindex series\n",
    "s1 = s1.reindex(index_union)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s2.reindex(index_union)\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Check where both series have NaNs\n",
    "nan_both = s1.isna() & s2.isna()\n",
    "nan_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Fill NaNs where only one series have missing data\n",
    "s1[~nan_both] = s1[~nan_both].fillna(0)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2[~nan_both] = s2[~nan_both].fillna(0)\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Carry out the operation\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 2.\n",
    "\n",
    "You are provided with simple numeric data frame (again!). Normalize it by rows and column with the Min-Max scaling.\n",
    "The lowest value should be $0$ and highest should be $1$.\n",
    "\n",
    "$$\\frac{X - \\text{Min}(X)}{\\text{Max}(X) - \\text{Min}(X)}$$\n",
    "\n",
    "Do it separately two times. Once normalize columns and then normalize rows. Do not normalize both rows and columns\n",
    "at the same time as with Min-Max scaling such operation does not make any sense! If you are curious you\n",
    "can do this and check how the data frame looks like in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "df = pd.DataFrame(np.random.normal(100, 15, (10, 3)), columns=['x', 'y', 'z'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 3.\n",
    "\n",
    "You are provided with a list of subject ids and two sets of measurements for those subjects from an experiment\n",
    "with two trials. However, some subjects may have participated in only one or even none of the trials.\n",
    "\n",
    "Your task is to compute average scores. For subjects with only one score the available score should be presented.\n",
    "Subjects with no data should be assigned with $-999$.\n",
    "\n",
    "HINT. You may want to use `.reindex()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(303)\n",
    "\n",
    "subj = pd.Index(range(30)) # List of subject ids\n",
    "x1 = pd.Series(np.random.normal(90, 10, (25,)), index=np.random.choice(subj, size=(25,), replace=False))\n",
    "x2 = pd.Series(np.random.normal(100, 15, (20,)), index=np.random.choice(subj, size=(20,), replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` | Column and row-wise operations aka _apply_\n",
    "\n",
    "We can do a lot with vectorization and labels alignment. However, sometimes we may want to apply arbitrary\n",
    "functions to columns or rows. For this we can use `.apply()` method.\n",
    "\n",
    "Apply is a sort of map-like statement in which one specifies a function that will be applied to every item\n",
    "(in this context items are rows or columns). Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "df = pd.DataFrame(np.random.normal(100, 15, (10, 3)), columns=['x', 'y', 'z'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithm to columns\n",
    "df.apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(len, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sum to columns\n",
    "df.apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df > 100].count()\n",
    "df.apply(lambda x: x[x > 100].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply as column filter\n",
    "df.apply(lambda x: x[x > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-trivial apply\n",
    "# Interquartile range per column\n",
    "#df.quantile(.75) - df.quantile(.25)\n",
    "\n",
    "df.apply(lambda x: x.quantile(.75) - x.quantile(.25))\n",
    "df.apply(lambda x: x.quantile(.75) - x.quantile(.25), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply function to rows. To do that we use additional `axis` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute row ranges\n",
    "df.apply(lambda x: x.max() - x.min(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with `.apply()` on columns the situation is simple as we can expect that our function will be applied to\n",
    "`Series` objects which are guaranteed to have fixed data types, so operations should be rather efficient.\n",
    "\n",
    "On the hand, it is not entirely clear what the representation of rows should be as they may contain values of\n",
    "different type. In general rows will also be represented as `Series` objects just such that are upcasted to an\n",
    "appropriate `dtype` that can store all the values. However, this means that row-wise apply will be very often\n",
    "much slower.\n",
    "\n",
    "We can see representation of a single item during apply by simply passing a function that prints items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation of columns\n",
    "_ = df.apply(print, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation of rows\n",
    "_ = df.apply(print, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases these are series with nice fixed dtype `float64`. But this is so only because the data frame we use\n",
    "is simple and contain only floating point numbers. See what happens when we really have mixed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sbn\n",
    "\n",
    "iris = sbn.load_dataset('iris')\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print row items\n",
    "_ = iris.head().apply(print, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are forced to work with `object` series. This will usually negatively impact efficiency of our computations.\n",
    "That is the reason why row apply is often slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `Series` are returned by function used in apply they will be combined to form a data frame.\n",
    "\n",
    "Below we show this with a non-trivial function that computes series with different fields for rows\n",
    "of the iris dataset based on the value of species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_func(row):\n",
    "    if row['species'] == 'setosa':\n",
    "        return pd.Series({'sepal': row['sepal_length'] / row['sepal_width']})\n",
    "    return pd.Series({'petal': row['petal_length'] / row['petal_width']})\n",
    "\n",
    "iris.apply(row_func, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames also define `.applymap()` method that applies a function element-wise.\n",
    "\n",
    "Below we use the method to find all the prime numbers in a data frame with non-negative integers.\n",
    "\n",
    "CAUTION. The implementation of the test for primality we use here is SUPER BAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(x):\n",
    "    if x < 2:\n",
    "        return False\n",
    "    for i in range(2, x):\n",
    "        if x % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "idf = pd.DataFrame(np.random.randint(0, 100, (10, 20)))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.applymap(is_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf[idf.applymap(is_prime)].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 4.\n",
    "\n",
    "You are provided with a data frame of exam scores of students. Each student have three exam scores ranging from 0 to 100.\n",
    "You have to convert scores to grades according to the grading scale below and compute average grades of students.\n",
    "\n",
    "**Grading scale**\n",
    "\n",
    "If score is:\n",
    "\n",
    "* $< 60 \\rightarrow 2$\n",
    "* $< 80 \\rightarrow 3$\n",
    "* $< 90 \\rightarrow 4$\n",
    "* $\\geq 90 \\rightarrow 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    pd.Series([61, 70, 90], name='Alice'),\n",
    "    pd.Series([50, 80, 91], name='Bob'),\n",
    "    pd.Series([80, 90, 82], name='Freya'),\n",
    "    pd.Series([90, 100, 92], name='Merlin')\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 5.\n",
    "\n",
    "Your are provided with a set of exam scores for $10$ students (in a form of a `dict`).\n",
    "Your task is to build a data frame in which row index values are student names,\n",
    "the first column stores exam scores and the second column stores grade according to the following rule:\n",
    "\n",
    "If score is:\n",
    "\n",
    "* $< 60 \\rightarrow 2$\n",
    "* $< 80 \\rightarrow 3$\n",
    "* $< 90 \\rightarrow 4$\n",
    "* $\\geq 90 \\rightarrow 5$\n",
    "\n",
    "HINT. You may want to convert the `dict` to a `Series` first. Then you can use `.apply()` or `.map()`\n",
    "to apply some computations to every element of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'Alice': 75,\n",
    "    'Bob': 80,\n",
    "    'Kate': 82,\n",
    "    'Dog': 99,\n",
    "    'Han Solo': 55,\n",
    "    'Rick': 100,\n",
    "    'Morty': 82,\n",
    "    'Santa': 62,\n",
    "    'Curie': 92,\n",
    "    'Isabelle': 88,\n",
    "    'Stan': 71,\n",
    "    'Kyle': 81,\n",
    "    'Kenny': 90,\n",
    "    'Cartman': 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` | Aggregation\n",
    "\n",
    "Data frames in Pandas offer a quite convenient interface for computing multiple aggregate quantities in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "\n",
    "iris = sbn.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[:, 'sepal_length':'petal_width'].agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.agg({\n",
    "    'sepal_length': [np.mean, np.std],\n",
    "    'species': 'nunique'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-apply-combine and `.groupby()`\n",
    "\n",
    "Split-apply-combine is a powerful strategy used frequently in data science, statistics and scientific computing.\n",
    "The main idea is that we can decompose even very complicate computations in a sequence of the three steps:\n",
    "\n",
    "* **Split.** In this step we split our dataset into smaller datasets based on some criterion, for instance based\n",
    "on value of some categorical variables.\n",
    "* **Apply.** In this stage a function or a set of functions is applied to datasets. In this step we often aggregate\n",
    "subdatasets into single values or perform some other processing (i.e. filtering).\n",
    "* **Combine.** In the end we combine splitted datasets back to one data frame.\n",
    "\n",
    "One of the typical usecases of this approach is computation of descriptive statistics for groups (i.e. group means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped operations in Pandas are defined via the `.groupby()` method. It returnes a special object\n",
    "that stores a dataset parts divided by a given criterion (usually by values of a column or columns).\n",
    "It can be used to iterate over the dataset parts but also to apply different functions to them.\n",
    "After a function is applied the grouped object will return results combined back to a single data frame.\n",
    "This is how _split-apply-combine_ strategy is implemented in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group iris data by species\n",
    "iris_g = iris.groupby(['species'])\n",
    "iris_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over groups and their names (criterion values)\n",
    "for name, group in iris_g:\n",
    "    print(name, \"\\n=========\\n\", group.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute group means\n",
    "iris.groupby('species')[['sepal_length', 'sepal_width']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard functions like mean can be used with even simpler syntax\n",
    "iris.groupby('species').apply(np.mean)\n",
    "iris.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use apply and transform methods with groupyby\n",
    "# Below we standardize numeric variables in groups\n",
    "z = iris \\\n",
    "    .groupby('species') \\\n",
    "    .apply(lambda gdf: (gdf - gdf.mean()) / gdf.std()) \\\n",
    "    .combine_first(iris)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check solution\n",
    "z.groupby('species').agg([np.mean, np.var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may use just a subset of columns\n",
    "Q = iris.groupby('species')['sepal_length'] \\\n",
    "    .apply(lambda x: x.quantile([0, .25, .5, .75, 1]))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And apply different functions to different columns\n",
    "iris.groupby('species').agg({\n",
    "    'sepal_length': [np.mean, np.std],\n",
    "    'petal_length': [np.mean, np.var],\n",
    "    'species': ['nunique']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course we can also ask for group sizes\n",
    "iris.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the methods above are useful but fall short when we need to perform complex computations depending on multiple\n",
    "columns in groups. Luckily, there is a trick that we can use while using the `.apply()` method that allows us\n",
    "to define arbitrary group computations using multiple columns as well as any other values we may need.\n",
    "\n",
    "Below we compute average of ratios of sepal / petal lengths and widths in groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.groupby('species') \\\n",
    "    .apply(lambda gdf: pd.Series({\n",
    "        'sepal': (gdf['sepal_length'] / gdf['sepal_width']).mean(),\n",
    "        'petal': (gdf['petal_length'] / gdf['petal_width']).mean()\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it is also possible to group by multiple columns.\n",
    "\n",
    "Below we show it using a famous dataset about Titanic survivors and casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sbn.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['class', 'embark_town']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame` | Exercise 6.\n",
    "\n",
    "Check how class and gender correlated with chance of survival in Titanic.\n",
    "\n",
    "HINT. Use `groupby` (duh!)\n",
    "\n",
    "HINT2. Note that the variable `survived` is simple binary vector, so you can compute its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sbn.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index hierarchies aka `MultiIndex`\n",
    "\n",
    "So far we limited our attention to data structures with flat indexes that have only one level and (usually) map one unique\n",
    "index value to one data value. But sometimes we may need something more elaborate. \n",
    "\n",
    "For instance, in one of the exercises we used a simple dataset with multiple exam scores for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    pd.Series([61, 70, 90], name='Alice'),\n",
    "    pd.Series([50, 80, 91], name='Bob'),\n",
    "    pd.Series([80, 90, 82], name='Freya'),\n",
    "    pd.Series([90, 100, 92], name='Merlin')\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas we may choose to represent the above data as a series object with two-level index that maps every data value\n",
    "to student name and the exam number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.stack()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use both index levels to address particular parts of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['Alice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['Alice':'Freya']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use values on two-levels. We pass it as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[('Merlin', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing on one of the nested level is a special kind of an operation that is called _cross-sectioning_. \n",
    "We have to use a special method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for second test\n",
    "s.xs(2, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily convert back and forth between multi index representation and simpler data frame representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = s.reset_index().rename(columns={'level_0': 'name', 'level_1': 'exam', 0: 'score'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.set_index(['name', 'exam'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[('Bob', 0):('Freya', 1), 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.xs(2, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the typical situation in which we find multi indexes are group by computations, in particular when we split\n",
    "by multiple columns. For instance, when we computed numbers of passengers from different ports by ticket class in Titanic dataset what we got was a series with two-level row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame with two-level row index\n",
    "titanic = sbn.load_dataset('titanic')\n",
    "\n",
    "gdf = titanic.groupby(['class', 'embark_town'])[['survived', 'fare']].mean()\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use indexes in group by. For this we specify `level` of the index that we will want to use to split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.groupby(level=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, mulit indexes are just sequences of unique tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create `MultiIndex` objects by hand from sequences of tuples or form a cartesian product of multiple sequences of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From list of tuples\n",
    "pd.MultiIndex.from_tuples([\n",
    "    ('a', 1),\n",
    "    ('a', 2),\n",
    "    ('b', 1),\n",
    "    ('b', 2),\n",
    "    ('b', 3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From cartesian product\n",
    "pd.MultiIndex.from_product([\n",
    "    ['a', 'b', 'c'], \n",
    "    [1, 2, 3, 4]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultiIndex` | Exercise 1.\n",
    "\n",
    "You are provided with set of multiple measurements (5) for ten persons. They are arranged in 10 by 5 Numpy array.\n",
    "Some value are missing (NaN). Your task is to arrange the data in a single `Series` object with index that differentiates\n",
    "properly between persons and measurements.\n",
    "\n",
    "Use your data structure to compute number of measurement per subject and number of subjects per measurements.\n",
    "\n",
    "Finally, use your data structure (without changing it) to compute\n",
    "average values by person and by measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.where(np.random.uniform(0, 1, (10, 5)) < .1, np.nan, np.random.normal(100, 15, (10, 5)))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
