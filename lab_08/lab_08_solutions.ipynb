{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Matplotlib | Some advanced features\n",
    "\n",
    "Szymon Talaga | 20.01.2020\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our last, final notebook. It reviews some (selected) advanced features of Pandas as well as introduce a better way to\n",
    "use Matplotlib visualization library.\n",
    "\n",
    "Remember that Pandas is quite vast a library, so we could not really cover everything. Luckily, the official documentation of Pandas\n",
    "is rather accessible and well written (for the most part). To solidify your knowledge it is recommended to read through:\n",
    "\n",
    "* [Getting started tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html)\n",
    "* [User guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n",
    "\n",
    "While reading you will notice that we managed to cover more or less all the core concepts, but in some cases we had to omit some details.\n",
    "We also did not talk at all about working with time series (at which Pandas, in fact, excels).\n",
    "\n",
    "The problems in the final homework will be as dependent only on topics that we discussed in class as possible. Thus, for instance\n",
    "there will be no questions about time series processing. However, it may happen that in some cases you will have to read a little bit\n",
    "on your own or at least consult a documentation page for a function or method. This is unavoidable and it is also what happens very often \n",
    "in any real project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Reading and writing from and to (text) files\n",
    "\n",
    "We already discussed this so this will be just a brief reminder.\n",
    "\n",
    "Reading from a text file can be done with `pd.read_csv()` function. \n",
    "In general it is quite complex (look at docpage with `pd.read_csv?`), but basic usage is rather simple.\n",
    "\n",
    "Similarily, you can write to a file with `.to_csv()` method. Again, it is quite complex but basic usage is simple.\n",
    "\n",
    "Now, we will first dump the _iris_ dataset to a CSV (comma-separated values) file and a TSV (tab-separated values) file with `.to_csv()`\n",
    "method and then we will read them back in with `.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset from Seaborn package\n",
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CSV (comma-separated)\n",
    "iris.to_csv('iris.csv', index=False)\n",
    "\n",
    "# Write TSV (tab-separated)\n",
    "# This time we additionally remove header with header=False argument\n",
    "iris.to_csv('iris.tsv', sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in from CSV\n",
    "iris_csv = pd.read_csv('iris.csv')\n",
    "iris_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in from TSV\n",
    "# First attempt\n",
    "pd.read_csv('iris.tsv', sep=\"\\t\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We forgot that we remove header\n",
    "pd.read_csv('iris.tsv', sep=\"\\t\", header=None).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may also pass column index by hand\n",
    "iris_tsv = pd.read_csv('iris.tsv', sep=\"\\t\", header=None, names=[\n",
    "    'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'\n",
    "])\n",
    "iris_tsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Sorting\n",
    "\n",
    "You can sort data frames (even by multiple columns) with `.sort_values()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101010)\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(0, 10, (15, 2)), columns=['x', 'y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['x', 'y'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Working with strings\n",
    "\n",
    "We can work quite efficiently with string columns thanks to the special `.str` attribute. It allows us to access standard string methods\n",
    "we know from standard Python and apply them in a vectorized manner to columns in data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].map(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].str.contains(r\"^v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only to species with names starting with 'v'\n",
    "iris.loc[iris['species'].str.contains(r\"^v\"), :]\n",
    "iris.loc[iris['species'].str.match(r\"^v\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"a string\"\n",
    "\n",
    "s.startswith('a')\n",
    "s.startswith('not a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to\n",
    "iris.loc[iris['species'].str.startswith('v'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Create new variables on the fly\n",
    "\n",
    "You can create a copy of a data frame with new columns (defined even according to complicated rules) with `.assign()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[iris['species'] == 'setosa', 'new_column'] = 999\n",
    "iris.head()\n",
    "iris.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.assign(\n",
    "    a_new_column = 1,\n",
    "    large_sepal_w = lambda df: df['sepal_width'] > 6\n",
    ")\n",
    "\n",
    "iris['large_seapal_w'] = iris['sepal_width'] > 6\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Method chaining syntax\n",
    "\n",
    "At this point we can see some pattern with respect to how Pandas works. We can notice that most of methods in Pandas return\n",
    "new data frame objects (usually copies). Thus, we can chain multiple method calls together one after another without creating\n",
    "any intermediate objects.\n",
    "\n",
    "We will see an example of this in which we wull compute again the `large_sepal_w` variable and then use it to compute distribution\n",
    "of species within the two groups defined by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1 = iris.assign(large_sepal_w = lambda df: df['sepal_length'] > 6)\n",
    "temp_df2 = temp_df1.groupby(['large_sepal_w'])\n",
    "temp_df3 = temp_df2.apply(lambda df: pd.Series(\n",
    "    df.groupby('species').size() / len(df),\n",
    "    name='freq'\n",
    "))\n",
    "temp_df3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris\n",
    "df.groupby('species').size() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method chainging syntax is most readable\n",
    "# when we separate every method call with new line and a proper indentation\n",
    "iris_dist = iris \\\n",
    "    .assign(large_sepal_l = lambda df: df['sepal_length'] > 6) \\\n",
    "    .groupby(['large_sepal_l']) \\\n",
    "    .apply(lambda df: pd.Series(\n",
    "        df.groupby('species').size() / len(df),\n",
    "        name='dist'\n",
    "    )) \\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas | Reshaping and pivoting, long and wide format\n",
    "\n",
    "One of the most important types of operations in practical data analysis is the ability to convert between long and wide\n",
    "formats of data frames.\n",
    "\n",
    "Long format is when one record (unit of observation) in a data frame corresponds not to a single subject, but to a single\n",
    "measurement for a subject. Thus, different variables measurements for a single subject are in individual rows.\n",
    "\n",
    "Wide format is when one record (unit of observation) in a data frame corresponds to a single subject and multiple \n",
    "variables/measurements are stored in different columns. For instance, the _iris_ datasets is by default represented\n",
    "using the wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we convert between wide format (above) and long format? The easiest way is to use `.stack()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_long = iris \\\n",
    "    .stack() \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\n",
    "        'level_0': 'idx',\n",
    "        'level_1': 'variable',\n",
    "        0: 'value'\n",
    "    })\n",
    "\n",
    "iris_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can undo the stacking operation with `.unstack()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris \\\n",
    "    .stack() \\\n",
    "    .unstack() \\\n",
    "    .eq(iris) \\\n",
    "    .all() \\\n",
    "    .all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we may want to convert to long format only for some subset of columns. For instance, it may be often useful\n",
    "(and we will soon see why) to gather only numeric columns this way and leave the `species` column in the wide format\n",
    "as a separate column.\n",
    "\n",
    "To do this we can use `.melt()` method which allows us to do exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_long = iris \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=['index', 'species']) \\\n",
    "    .sort_values(by='index') \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "iris_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we now go back to the wide format? It turns out we can do that quite easily by using a few simple\n",
    "indexing tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_long.groupby('index')['species'].apply(lambda x: x.unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_wide = iris_long \\\n",
    "    .set_index(['index', 'variable']) \\\n",
    "    .loc[:, 'value'] \\\n",
    "    .unstack() \\\n",
    "    .merge(\n",
    "        iris_long.groupby('index')['species'].agg(lambda x: x.iloc[0]),\n",
    "        right_index=True,\n",
    "        left_index=True\n",
    "    ) \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "iris_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation, merging and relational operations\n",
    "\n",
    "The last Pandas topic that we will discuss is how to join different data frames together.\n",
    "Since data frames are 2-dimensional then we can think about this problem in several different ways.\n",
    "\n",
    "We may want to join multiple frames side-by-side or stack them one on top of another.\n",
    "Moreover, we may also think about different kinds of so-called relational operations (joins) in which\n",
    "we add new columns in a host data frame based on columns of another data frame while aligning rows\n",
    "by a specified key columns (or indexes).\n",
    "\n",
    "Let us now review these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [3, 4, 5]\n",
    "})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'a': [1, 7],\n",
    "    'b': [10, 11]\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical stacking \n",
    "pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical stacking with new indexes\n",
    "pd.concat([df1, df2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2, 3, 4]\n",
    "})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'c': [1, 4, 5],\n",
    "    'd': [4, 6, 9]\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join\n",
    "\n",
    "Left join is one of the most fundamental data processing operations. It allows us to add column to a data frame based on values\n",
    "in another data frame in such a way that values between the two data frames are aligned according to a prespecified key column(s).\n",
    "\n",
    "Let us see this on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'key': ['a', 'b', 'c', 'a', 'a', 'c', 'f'],\n",
    "    'x': [1, 2, 3, 4, 5, 6, 100]\n",
    "})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'key': ['a', 'b', 'c', 'd'],\n",
    "    'y': [1, 2, 3, 11]\n",
    "})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform left join between the two data frames with `.merge()` method.\n",
    "Merge is quite flexible. You can learn more from the [official docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, how='left', on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform an inner join to extract only records that have matching keys in both data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, how='inner', on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can perform an outer join to extract all records, even the ones without a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, how='outer', on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "Use _iris_ dataset and add to it a new column called `sepal_length_mean` in which you compute store average values divided by species.\n",
    "The output dataset should still have 150 rows. In other words for every record of a given species it should store mean value\n",
    "of `sepal_length` for this species.\n",
    "\n",
    "HINT. You try to use `groupby` and `merge` methods to do this quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "\n",
    "# Version one\n",
    "iris.merge(\n",
    "    pd.Series(iris.groupby('species')['sepal_length'].mean(), name='sepal_length_mean'),\n",
    "    how='left',\n",
    "    left_on='species',\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "# Version two\n",
    "iris \\\n",
    "    .groupby('species') \\\n",
    "    .apply(lambda df: df.assign(\n",
    "        sepal_length_mean = df['sepal_length'].mean()\n",
    "    )) \\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. \n",
    "\n",
    "Here you will work with a famous `mpg` dataset. It contains some technical information about few hundred models of cars.\n",
    "You want to compute mean and medians of all numeric variables divided into groups according to origin.\n",
    "However, you want to have individual variables in rows, not in columns.\n",
    "\n",
    "So first you need to use `.melt()` method to convert `mpg` to the long format while also leaving `origin` as a separate column.\n",
    "You will also have to drop `name` column.\n",
    "\n",
    "Write your solution nicely using the method chaining syntax.\n",
    "\n",
    "Try to structure your output in such a way as to facilitate comparisons between regions (values of `origin`).\n",
    "\n",
    "HINT. You may look at the `.drop` method. It may allow you to drop `name` column very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sbn.load_dataset('mpg')\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "\n",
    "# Version 1.\n",
    "mpg \\\n",
    "    .drop('name', axis='columns') \\\n",
    "    .reset_index() \\\n",
    "    .melt(id_vars=['index', 'origin']) \\\n",
    "    .drop('index', axis=1) \\\n",
    "    .groupby(['origin', 'variable']) \\\n",
    "    .agg([np.mean, np.median])\n",
    "\n",
    "# Version 2.\n",
    "mpg \\\n",
    "    .groupby('origin') \\\n",
    "    .agg([np.mean, np.median]) \\\n",
    "    .T\n",
    "\n",
    "## The second solution is arguably much nicer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib | Object-oriented API\n",
    "\n",
    "So far we used Matplotlib in a slightly naive way, which is okay for simple tasks, but sooner or later becomes problematic.\n",
    "What is the problem here?\n",
    "\n",
    "The problem is that we used so-called (stateful) Pylab API. It means that to draw every plot we have been using functions\n",
    "defined globally at the package level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sbn\n",
    "\n",
    "# Set Matplotlib style\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (100,))\n",
    "Y = np.random.normal(0, 1, (100,))\n",
    "\n",
    "_ = plt.hist(X)\n",
    "_ = plt.hist(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the example above shows, in this approach every action that we perform is _added up_ to every previous action.\n",
    "As a result, the Pylab API may be quite unpredictable when used for more complex tasks as earlier actions may affect\n",
    "later actions in surprising way.\n",
    "\n",
    "It also make it impossible to create multiple plots in a single chunk of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is why in general it is prefereable to work with the object-oriented API. In this approach every figure\n",
    "we draw is represented by a single `Figure` object and one or more `Axes` objects.\n",
    "\n",
    "`Figure`s represent entire figures and `Axes` represent particular panels (windows) on a figure.\n",
    "We will see what that means exactly just in few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot with Pyplot API\n",
    "for group, df in iris.groupby('species'):\n",
    "    _ = plt.scatter(df['sepal_length'], df['sepal_width'], label=group)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central function in the object-oriented approach is `plt.subplots`.\n",
    "In the simplest case it creates a single `Figure` object and a single `Axes` object that jointly\n",
    "correspond to a single figure with just one plot panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot with object-oriented API\n",
    "fig, ax = plt.subplots()\n",
    "for group, df in iris.groupby('species'):\n",
    "    _ = ax.scatter(df['sepal_length'], df['sepal_width'], label=group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what did we gain here? The good thing is that we can redraw the figure multiple times in different codes chunks.\n",
    "And if we want we can modify it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a `Figure` object to save any figure we created at any point in time.\n",
    "With Pyplot API we can save only the most recent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as PDF\n",
    "fig.savefig('fig1.pdf', bbox_inches='tight')\n",
    "# Save as PNG\n",
    "fig.savefig('fig1.png', bbox_inches='tight')\n",
    "\n",
    "# I usually use option `bbox_inches='tight'`\n",
    "# to leave less whitespace around a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the main idea is that we perform top-level operations such as assigning global style or\n",
    "saving a figure using `Figure` objects and we draw particular (sub)plots using `Axes` objects.\n",
    "\n",
    "For instance, once a plot is drawn, we can still modify it and for instance add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styles\n",
    "\n",
    "Matplotlib allows to set very detailed style configurations.\n",
    "However, configuring styling by hand is usually quite difficult, so it is rather recommended\n",
    "to use one of the [built-in styles](https://matplotlib.org/3.1.0/gallery/style_sheets/style_sheets_reference.html).\n",
    "\n",
    "They can be turn on with the following simple function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use globally seaborn-white style\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about details you may learn more about styling in Matplotlib [here](https://matplotlib.org/tutorials/introductory/customizing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-panel plots\n",
    "\n",
    "In data analysis we often want to visualize realtionships between multiple variables at one plot.\n",
    "That is why we have a notion of a plot panel (or a subplot). It is a sort of a single window or canvas\n",
    "on which, for instance, scatter plot between a pair of variables may be shown. Thus, we can use multiple\n",
    "panels to show jointly many pairwise relationship between variables.\n",
    "\n",
    "Below we draw a matrix of pairwise scatter plots for all numeric variables from _iris_ dataset.\n",
    "We have four variables so we use a 4-by-4 grid of subplots. Along the diagonal we have plots with\n",
    "the same variable on both `x` and `y` axis, so in this case we will show a histogram.\n",
    "This way we will see not only pairwise relationships between all variables, but one-dimensional\n",
    "distributions of all variables.\n",
    "\n",
    "We will also group everything by species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sbn.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "variables = iris.loc[:, 'sepal_length':'petal_width'].columns\n",
    "# Grouped iris\n",
    "iris_g = iris.groupby('species')\n",
    "\n",
    "# Initialize figure with 4-by-4 grid of subplot axes objects\n",
    "# We also set larger figure size\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 12))\n",
    "\n",
    "for ax, xy in zip(axes.flatten(), product(variables, variables)):\n",
    "    # Unpack 2-tuple with variable names\n",
    "    x, y = xy\n",
    "    for group, df in iris_g:\n",
    "        # Set X-axis label\n",
    "        _ = ax.set_xlabel(x)\n",
    "        if x == y:\n",
    "            _ = ax.hist(df[x], label=group)\n",
    "        else:\n",
    "            _ = ax.scatter(df[x], df[y], label=group)\n",
    "            # Set Y-axis labels\n",
    "            _ = ax.set_ylabel(y)\n",
    "\n",
    "# We may want to add legend only on one of the subplots\n",
    "_ = axes.flatten()[2].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "\n",
    "Use `mpg` dataset.\n",
    "\n",
    "Draw scatter plot of `mpg` and `weight` with different colors for different production regions (`origin`).\n",
    "\n",
    "Use object-oriented matplotlib API.\n",
    "\n",
    "Save the figure as PDF file named `mpg.pdf`. Check that it looks correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sbn.load_dataset('mpg')\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x, y = 'mpg', 'weight'\n",
    "for name, df in mpg.groupby('origin'):\n",
    "    _ = ax.scatter(df[x], df[y], label=name)\n",
    "    _ = ax.set_xlabel(x)\n",
    "    _ = ax.set_ylabel(y)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.\n",
    "\n",
    "Use `mpg` dataset.\n",
    "\n",
    "Draw 3-by-3 matrix of scatter plots (with histograms along the diagonal) showing relationships between\n",
    "`mpg`, `weight` and `horsepower`. Use different colors for different regions (`origin` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sbn.load_dataset('mpg')\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "from itertools import product\n",
    "\n",
    "cols = ['mpg', 'weight', 'horsepower']\n",
    "n = len(cols)\n",
    "# Grouped dataset\n",
    "mpg_g = mpg.groupby('origin')\n",
    "# Initialize figure and axes\n",
    "fig, axes = plt.subplots(nrows=n, ncols=n, figsize=(15, 12))\n",
    "\n",
    "for ax, xy in zip(axes.flatten(), product(cols, cols)):\n",
    "    x, y = xy   # Unpack tuple with variable names\n",
    "    _ = ax.set_xlabel(x)\n",
    "    for name, df in mpg_g:\n",
    "        if x == y:\n",
    "            _ = ax.hist(df[x], label=name, alpha=.9)\n",
    "        else:\n",
    "            _ = ax.scatter(df[x], df[y], label=name, alpha=.9)\n",
    "            _ = ax.set_ylabel(y)\n",
    "            \n",
    "# Add legend in a convenient spot\n",
    "_ = axes[0, 1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn package\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/) is a data visualization library built on top of Matplotlib.\n",
    "It makes drawing few preselected types of plots very easy, but in my experience it is very opinionated\n",
    "and makes it almost impossible to draw beautiful custom plots as it puts a lot of constraints on the user.\n",
    "Because of that (and also because of the lack of time) we will not discuss Seaborn in the class.\n",
    "However, if you are interested you may give it a try. Also, Seaborn-based solutions in HW3 problems\n",
    "focused on data visualization will be accepted as long as they are correct. In other words,\n",
    "you may choose wheter you want to use pure Matplotlib or Seaborn for data visualizations in HW3.\n",
    "\n",
    "https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
